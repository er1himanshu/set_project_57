# CLIP Configuration
# ====================

# Base CLIP model to use (from HuggingFace)
# Options: openai/clip-vit-base-patch32 (default, fastest)
#          openai/clip-vit-base-patch16 (more accurate)
#          openai/clip-vit-large-patch14 (best accuracy, slowest)
CLIP_MODEL_NAME=openai/clip-vit-base-patch32

# Path to fine-tuned CLIP model (optional)
# If set, this will be used instead of CLIP_MODEL_NAME
# Example: /path/to/fine_tuned_clip/best_model
CLIP_FINE_TUNED_MODEL_PATH=

# Similarity threshold for mismatch detection (0.0-1.0)
# Lower values = more sensitive (catches more mismatches, more false positives)
# Higher values = less sensitive (fewer false positives, may miss mismatches)
# Recommended: 0.25-0.3 for balanced performance
CLIP_SIMILARITY_THRESHOLD=0.25

# Device to use for CLIP inference
# Options: cpu (slower, no GPU required)
#          cuda (faster, requires NVIDIA GPU with CUDA)
# Default: cpu (will auto-detect CUDA if available)
CLIP_DEVICE=cpu

# Cache directory for downloaded CLIP models
# Models are ~350MB-1.7GB depending on variant
# Default: backend/clip_models
# CLIP_CACHE_DIR=/path/to/cache

# Database Configuration
# ======================
DATABASE_URL=sqlite:///./image_quality.db

# File Upload Configuration
# ==========================
MAX_FILE_SIZE=10485760  # 10MB in bytes
UPLOAD_DIR=./uploads

# Quality Thresholds
# ==================
MIN_WIDTH=1000
MIN_HEIGHT=1000
BLUR_THRESHOLD=100.0
MIN_BRIGHTNESS=60
MAX_BRIGHTNESS=200
MIN_SHARPNESS=50.0
MIN_BACKGROUND_SCORE=0.7

# Server Configuration
# ====================
# HOST=0.0.0.0
# PORT=8000
