# AI-Powered Ecommerce Product Listing Evaluator

A full-stack application for analyzing product images with comprehensive AI-powered quality metrics specifically designed for ecommerce marketplaces. Ensure your product images meet professional standards with instant feedback and actionable suggestions.

## ğŸŒŸ Features

### Image Quality Analysis
- **Resolution Validation**: Ensures images meet minimum 1000Ã—1000 pixel requirement
- **Blur Detection**: Uses Laplacian variance to detect unfocused or blurry images
- **Sharpness Analysis**: Edge detection to verify image clarity
- **Brightness & Contrast**: Validates proper lighting conditions
- **Aspect Ratio Check**: Validates standard ecommerce ratios (1:1, 4:3, 16:9, etc.)

### Ecommerce Standards
- **Background Assessment**: Scores background cleanliness for white/neutral backgrounds
- **Watermark Detection**: Identifies text overlays and watermarks
- **Description Consistency**: Validates alignment between product description and image content (color matching, basic heuristics)
- **ğŸ†• Image-Text Mismatch Detection**: Uses pretrained CLIP (Contrastive Language-Image Pre-training) model to detect mismatches between product images and descriptions with AI-powered similarity scoring
- **ğŸ†• CLIP Explainability**: Generates attention-based heatmap overlays showing which regions of the image contribute most to the similarity score, helping you understand what the AI model focuses on

### User Experience
- **Modern UI**: Beautiful gradient-based design with card layouts
- **Image Preview**: See your image before upload
- **Real-time Results**: Instant analysis with detailed quality checklist
- **ğŸ†• Visual Explanations**: Interactive "Show Explanation" button to display attention heatmaps over images
- **Improvement Suggestions**: Actionable tips to enhance image quality
- **Results Dashboard**: Track all analyses with statistics and filters

## ğŸš€ Prerequisites

- **Python** 3.8 or higher
- **Node.js** 16 or higher
- **pip** (Python package manager)
- **npm** (Node package manager)

## ğŸ“¦ Installation & Setup

### 1. Clone the Repository

```bash
git clone https://github.com/er1himanshu/set_project_57.git
cd set_project_57
```

### 2. Backend Setup (FastAPI)

```bash
# Navigate to backend directory
cd backend

# Create and activate virtual environment (recommended)
python3 -m venv venv

# On macOS/Linux:
source venv/bin/activate

# On Windows:
venv\Scripts\activate

# Install dependencies (this may take several minutes for CLIP model dependencies)
pip install -r requirements.txt

# Start the backend server
uvicorn app.main:app --reload
```

**Note**: The first time you upload an image with a description, the CLIP model (~350MB) will be automatically downloaded. This is a one-time download.

The backend API will run at **http://localhost:8000**
- Interactive API documentation: **http://localhost:8000/docs**
- Alternative API docs: **http://localhost:8000/redoc**

### 3. Frontend Setup (React + Vite)

Open a new terminal window:

```bash
# Navigate to frontend directory
cd frontend

# Install dependencies
npm install

# Start the development server
npm run dev
```

The frontend will run at **http://localhost:5173**

## ğŸ¯ Usage

### Uploading Images for Analysis

1. Navigate to **http://localhost:5173** in your web browser
2. Click the upload area or drag and drop a product image
3. (Optional) Enter a product description for consistency checking
4. Click "Analyze Image Quality"
5. View detailed results with pass/fail status, quality metrics, and improvement suggestions

### ğŸ†• Using CLIP Explainability (Show Explanation)

1. Upload an image and enter a product description (at least 10 characters)
2. Click the **"Show Explanation"** button
3. The system generates an attention-based heatmap showing which regions of the image the AI focuses on when matching it to your description
4. The heatmap overlay is displayed on the image, with warmer colors (red/yellow) indicating areas of high attention
5. View the similarity score to understand how well the image matches the description
6. Click **"Hide Explanation"** to return to the original image view

**Note**: The explainability feature requires the CLIP model to be available. In offline or sandboxed environments where the model cannot be downloaded, this feature will be unavailable but won't affect other functionality.

### Viewing Results

- Click "Results" in the navigation bar to see all analyzed images
- View statistics: total analyzed, passed, and failed counts
- Expand suggestions for failed images to see improvement tips

## ğŸ“¡ API Endpoints

### Upload Image
```http
POST /upload
Content-Type: multipart/form-data

Parameters:
  - file: image file (required)
  - description: product description text (optional, enables mismatch detection)

Response:
{
  "message": "Uploaded & analyzed",
  "result_id": 1,
  "passed": true
}
```

**Note**: When a description is provided, the system automatically runs CLIP-based mismatch detection alongside quality checks.

### Check Image-Text Mismatch
```http
POST /check-mismatch
Content-Type: multipart/form-data

Parameters:
  - file: image file (required)
  - description: product description text (required, min 10 characters)
  - threshold: similarity threshold 0-1 (optional, default: 0.25)

Response:
{
  "filename": "product_abc123.jpg",
  "description": "Red leather handbag",
  "has_mismatch": false,
  "similarity_score": 0.85,
  "threshold": 0.25,
  "message": "Match confirmed (score: 0.85)",
  "recommendation": "Image and description match well."
}
```

**Use this endpoint** when you specifically want to test mismatch detection without storing results in the database.

### ğŸ†• Explain Image-Text Similarity (CLIP Explainability)
```http
POST /explain
Content-Type: multipart/form-data

Parameters:
  - file: image file (required, max 10MB, jpg/png/gif/webp)
  - description: product description text (required, min 10 characters)

Response:
{
  "filename": "product_abc123.jpg",
  "description": "Red leather handbag",
  "heatmap_base64": "iVBORw0KGgoAAAANSUhEUgAA...",
  "similarity_score": 0.85,
  "message": "Explanation generated (similarity: 0.85)"
}
```

**Use this endpoint** to generate visual explanations of CLIP's image-text similarity using attention rollout. The heatmap overlay shows which regions of the image contribute most to the similarity score, helping you understand what the AI model is focusing on when matching images to descriptions.

The `heatmap_base64` field contains a base64-encoded PNG image with the attention heatmap overlaid on the original image. You can display it directly in HTML using:
```html
<img src="data:image/png;base64,{heatmap_base64}" />
```

### Get All Results
```http
GET /results

Response: Array of ImageResultSchema objects
```

### Get Specific Result
```http
GET /results/{result_id}

Response: ImageResultSchema object with all quality metrics
```

### Get Analysis by Image ID
```http
GET /analyze/{image_id}

Response: ImageResultSchema object
```

## ğŸ“Š Quality Criteria

Images are evaluated against the following ecommerce standards:

| Metric | Requirement | Description |
|--------|-------------|-------------|
| **Resolution** | â‰¥ 1000Ã—1000 px | Minimum pixel dimensions |
| **Blur Score** | â‰¥ 100.0 | Laplacian variance threshold |
| **Sharpness** | â‰¥ 50.0 | Edge detection score |
| **Brightness** | 60-200 | Optimal lighting range |
| **Aspect Ratio** | Standard ratios | 1:1, 4:3, 3:4, 16:9, 9:16 (Â±10% tolerance) |
| **Background** | â‰¥ 70% | Clean/white background score |
| **Watermarks** | None | No text overlays or watermarks |
| **Description** | Consistent | Color and content matching |
| **ğŸ†• Image-Text Match** | â‰¥ 0.25 | CLIP similarity score (0-1) |

### Response Schema

```json
{
  "id": 1,
  "filename": "product.jpg",
  "width": 1200,
  "height": 1200,
  "blur_score": 150.5,
  "brightness_score": 120.3,
  "contrast_score": 45.2,
  "passed": true,
  "reason": "OK",
  "description": "Red leather handbag",
  "aspect_ratio": 1.0,
  "sharpness_score": 85.7,
  "background_score": 0.85,
  "has_watermark": false,
  "description_consistency": "Consistent",
  "improvement_suggestions": "Image meets quality standards",
  "has_mismatch": false,
  "similarity_score": 0.85,
  "mismatch_message": "Match confirmed (score: 0.85)"
}
```

## ğŸ—ï¸ Project Structure

```
set_project_57/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI application entry point
â”‚   â”‚   â”œâ”€â”€ models.py            # SQLAlchemy database models
â”‚   â”‚   â”œâ”€â”€ schemas.py           # Pydantic validation schemas
â”‚   â”‚   â”œâ”€â”€ config.py            # Configuration and thresholds
â”‚   â”‚   â”œâ”€â”€ database.py          # Database setup and connection
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ upload.py        # Image upload endpoint
â”‚   â”‚   â”‚   â”œâ”€â”€ analyze.py       # Analysis endpoint
â”‚   â”‚   â”‚   â”œâ”€â”€ mismatch.py      # ğŸ†• Mismatch detection endpoint
â”‚   â”‚   â”‚   â””â”€â”€ results.py       # Results retrieval endpoints
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â”œâ”€â”€ image_quality.py # Image analysis logic
â”‚   â”‚       â”œâ”€â”€ mismatch_detector.py # ğŸ†• CLIP-based mismatch detection
â”‚   â”‚       â””â”€â”€ storage.py       # File storage management
â”‚   â”œâ”€â”€ requirements.txt         # Python dependencies
â”‚   â””â”€â”€ uploads/                 # Uploaded images (auto-created)
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.jsx    # Home page with upload
â”‚   â”‚   â”‚   â””â”€â”€ Results.jsx      # Results listing page
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Navbar.jsx       # Navigation component
â”‚   â”‚   â”‚   â”œâ”€â”€ UploadForm.jsx   # Upload and analysis UI
â”‚   â”‚   â”‚   â””â”€â”€ ResultsTable.jsx # Results display table
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ client.js        # API client
â”‚   â”‚   â”œâ”€â”€ App.jsx              # Main app component
â”‚   â”‚   â””â”€â”€ main.jsx             # Entry point
â”‚   â”œâ”€â”€ index.html               # HTML template
â”‚   â”œâ”€â”€ package.json             # Node dependencies
â”‚   â””â”€â”€ tailwind.config.js       # Tailwind CSS config
â””â”€â”€ README.md                    # This file
```

## ğŸ”§ Configuration

### Backend Configuration

Edit `backend/app/config.py` to adjust quality thresholds:

```python
MIN_WIDTH = 1000              # Minimum image width
MIN_HEIGHT = 1000             # Minimum image height
BLUR_THRESHOLD = 100.0        # Blur detection threshold
MIN_SHARPNESS = 50.0          # Sharpness threshold
MIN_BRIGHTNESS = 60           # Minimum brightness
MAX_BRIGHTNESS = 200          # Maximum brightness
MIN_BACKGROUND_SCORE = 0.7    # Background quality threshold

# ğŸ†• Image-Text Mismatch Detection
MISMATCH_THRESHOLD = 0.25     # Similarity threshold (0-1, lower = stricter)
CLIP_MODEL_NAME = "openai/clip-vit-base-patch32"  # CLIP model to use
```

**Note**: The image-text mismatch detection feature requires the CLIP model to be available locally. If the model is not cached and cannot be downloaded (e.g., in offline/sandboxed environments), the feature will be gracefully skipped and uploads will continue successfully without mismatch detection. The image quality analysis will still be performed normally.

### Frontend Configuration

Edit `frontend/src/api/client.js` to change the API endpoint:

```javascript
const API = axios.create({
  baseURL: "http://localhost:8000"  // Backend URL
});
```

## ğŸ§ª Example Requests

### Using cURL

```bash
# Upload with description (includes automatic mismatch detection)
curl -X POST http://localhost:8000/upload \
  -F "file=@/path/to/product.jpg" \
  -F "description=Red leather handbag with gold hardware"

# ğŸ†• Check mismatch detection specifically
curl -X POST http://localhost:8000/check-mismatch \
  -F "file=@/path/to/product.jpg" \
  -F "description=Red leather handbag with gold hardware"

# ğŸ†• Check mismatch with custom threshold
curl -X POST http://localhost:8000/check-mismatch \
  -F "file=@/path/to/product.jpg" \
  -F "description=Red leather handbag" \
  -F "threshold=0.30"

# ğŸ†• Get CLIP explainability heatmap
curl -X POST http://localhost:8000/explain \
  -F "file=@/path/to/product.jpg" \
  -F "description=Red leather handbag with gold hardware" \
  -o explanation_result.json

# Get all results
curl http://localhost:8000/results

# Get specific result
curl http://localhost:8000/results/1
```

### Using Python

```python
import requests

# Upload image with description (includes automatic mismatch detection)
url = "http://localhost:8000/upload"
files = {"file": open("product.jpg", "rb")}
data = {"description": "Blue cotton t-shirt"}
response = requests.post(url, files=files, data=data)
print(response.json())

# ğŸ†• Check mismatch detection specifically
url = "http://localhost:8000/check-mismatch"
files = {"file": open("product.jpg", "rb")}
data = {"description": "Blue cotton t-shirt"}
response = requests.post(url, files=files, data=data)
result = response.json()
print(f"Has Mismatch: {result['has_mismatch']}")
print(f"Similarity Score: {result['similarity_score']}")
print(f"Message: {result['message']}")

# ğŸ†• Get CLIP explainability heatmap
url = "http://localhost:8000/explain"
files = {"file": open("product.jpg", "rb")}
data = {"description": "Blue cotton t-shirt"}
response = requests.post(url, files=files, data=data)
result = response.json()
print(f"Similarity Score: {result['similarity_score']}")
print(f"Message: {result['message']}")
# Save the heatmap image
import base64
with open("heatmap.png", "wb") as f:
    f.write(base64.b64decode(result['heatmap_base64']))
print("Heatmap saved to heatmap.png")

# Get results
results = requests.get("http://localhost:8000/results")
print(results.json())
```

## ğŸ› ï¸ Technology Stack

### Backend
- **FastAPI**: Modern Python web framework
- **SQLAlchemy**: SQL toolkit and ORM
- **OpenCV**: Computer vision and image processing
- **NumPy**: Numerical computing
- **scikit-image**: Image processing algorithms
- **ğŸ†• Transformers**: Hugging Face library for pretrained models
- **ğŸ†• PyTorch**: Deep learning framework for CLIP model
- **ğŸ†• Pillow**: Python Imaging Library for image handling

### Frontend
- **React 18**: UI library
- **Vite**: Build tool and dev server
- **Tailwind CSS**: Utility-first CSS framework
- **Axios**: HTTP client
- **React Router**: Client-side routing

## ğŸ› Troubleshooting

### Backend Issues

**Port already in use:**
```bash
# Use a different port
uvicorn app.main:app --reload --port 8001
```

**Database issues:**
```bash
# Remove the database file and restart
rm image_quality.db
uvicorn app.main:app --reload
```

**OpenCV import errors:**
```bash
# Install system dependencies (Ubuntu/Debian)
sudo apt-get install python3-opencv libgl1

# Or reinstall opencv-python
pip uninstall opencv-python
pip install opencv-python
```

**ğŸ†• CLIP model download issues:**
```bash
# The CLIP model (~350MB) downloads automatically on first use
# If download fails, check your internet connection and try again

# To pre-download the model:
python -c "from transformers import CLIPModel, CLIPProcessor; CLIPModel.from_pretrained('openai/clip-vit-base-patch32'); CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')"
```

**ğŸ†• PyTorch installation issues:**
```bash
# If torch installation fails, visit https://pytorch.org/get-started/locally/
# for platform-specific installation instructions

# For CPU-only installation:
pip install torch --index-url https://download.pytorch.org/whl/cpu
```

### Frontend Issues

**Port 5173 already in use:**
```bash
# Kill the process
lsof -ti:5173 | xargs kill -9

# Or use different port
npm run dev -- --port 3000
```

**CORS errors:**
- Ensure backend is running at http://localhost:8000
- Check CORS settings in `backend/app/main.py`

## ğŸ“ License

This project is open source and available for educational purposes.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Support

For issues and questions, please open an issue on the GitHub repository.